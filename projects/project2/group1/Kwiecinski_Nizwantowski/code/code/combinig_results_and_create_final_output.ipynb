{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from metrics import default_competition_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"../data/x_train.npy\")\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "X_val = np.load(\"../data/x_val.npy\")\n",
    "y_val = np.load(\"../data/y_val.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual model\n",
    "we were not sure, which subset of features should we use: [100, 102, 103, 105] or [100, 102, 105] till the very end, so we evaluated them with details and mds.\n",
    "\n",
    "### testing on features [100, 102, 103, 105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    \"n_estimators\": 1600,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 4,\n",
    "    \"max_features\": \"log2\",\n",
    "    \"max_depth\": 10,\n",
    "    \"bootstrap\": False,\n",
    "}\n",
    "svm_params = {\n",
    "    'kernel': 'rbf',\n",
    "    'gamma': 0.01,\n",
    "    'C': 1,\n",
    "}\n",
    "xgboost_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.01,\n",
    "    'colsample_bytree': 0.75,\n",
    "}\n",
    "features = [100, 102, 103, 105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:, features]\n",
    "X_val = X_val[:, features]\n",
    "number_of_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_X = X_train\n",
    "train_set_y = y_train\n",
    "val_set_X = X_val\n",
    "val_set_y = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(\n",
    "    n_components=1,\n",
    "    random_state=21,\n",
    "    n_init=6,\n",
    "    normalized_stress=False,\n",
    "    eps=0.0001,\n",
    "    max_iter=600,\n",
    ")\n",
    "# this step should be calculated in every cross-validation fold so 5 * 10 = 50 times\n",
    "# but it takes a lot of time so we will do it only once and our results will be biased\n",
    "# but in validation it is done by the book\n",
    "train_mds = mds.fit_transform(train_set_X)\n",
    "val_mds = mds.fit_transform(val_set_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_X_with_mds = np.concatenate([train_set_X, train_mds], axis=1)\n",
    "val_set_X_with_mds = np.concatenate([val_set_X, val_mds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/mds_train_103.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_mds, f)\n",
    "    \n",
    "with open(\"../data/mds_val_103.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_mds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/mds_train_103.pkl\", \"rb\") as f:\n",
    "    train_mds = pickle.load(f)\n",
    "with open(\"../data/mds_val_103.pkl\", \"rb\") as f:\n",
    "    val_mds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing on cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:  6745.833333333333 85.18770125108696\n",
      "SVM:  6962.5 27.638539919628332\n",
      "RF:  6515.277777777777 53.93468844152111\n",
      "Ensemble:  6834.722222222223 44.00827643820794\n"
     ]
    }
   ],
   "source": [
    "seeds = [21, 22, 23, 24, 25, 5111, 23525, 34934, 343243]\n",
    "rf_results = []\n",
    "svm_results = []\n",
    "xgboost_results = []\n",
    "ensemble_results = []\n",
    "\n",
    "for num, seed in enumerate(seeds):\n",
    "    cv_folds = 5\n",
    "    np.random.seed(seed)\n",
    "    idx = np.array(range(X_train.shape[0]))\n",
    "    np.random.shuffle(idx)\n",
    "    rf = []\n",
    "    svm = []\n",
    "    xgboost = []\n",
    "    ensemble = []\n",
    "\n",
    "    idx = idx % cv_folds\n",
    "    for j in range(cv_folds):\n",
    "        X_train_cv = train_set_X_with_mds[idx != j]\n",
    "        X_val_cv = train_set_X_with_mds[idx == j]\n",
    "        X_train_cv_with_mds = train_set_X_with_mds[idx != j]\n",
    "        X_val_cv_with_mds = train_set_X_with_mds[idx == j]\n",
    "        y_train_cv = y_train[idx != j]\n",
    "        y_val_cv = y_train[idx == j]\n",
    "\n",
    "        model1 = xgb.XGBClassifier(**xgboost_params)\n",
    "        model1.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        model2 = SVC(probability=True, **svm_params)\n",
    "        model2.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        model3 = RandomForestClassifier(**rf_params)\n",
    "        model3.fit(X_train_cv_with_mds, y_train_cv)\n",
    "\n",
    "        predictions1 = model1.predict_proba(X_val_cv)[:, 1]\n",
    "        predictions2 = model2.predict_proba(X_val_cv)[:, 1]\n",
    "        predictions3 = model3.predict_proba(X_val_cv_with_mds)[:, 1]\n",
    "        final_predictions_proba = (predictions1 + predictions2 + predictions3) / 3\n",
    "\n",
    "        xgboost.append(\n",
    "            default_competition_metric(\n",
    "                y_val_cv, y_pred_proba=predictions1, k=number_of_features\n",
    "            )\n",
    "        )\n",
    "        svm.append(\n",
    "            default_competition_metric(\n",
    "                y_val_cv, y_pred_proba=predictions2, k=number_of_features\n",
    "            )\n",
    "        )\n",
    "        rf.append(\n",
    "            default_competition_metric(\n",
    "                y_val_cv, y_pred_proba=predictions3, k=number_of_features\n",
    "            )\n",
    "        )\n",
    "        ensemble.append(\n",
    "            default_competition_metric(\n",
    "                y_val_cv, y_pred_proba=final_predictions_proba, k=number_of_features\n",
    "            )\n",
    "        )\n",
    "\n",
    "    xgboost_results.append(np.mean(xgboost))\n",
    "    svm_results.append(np.mean(svm))\n",
    "    rf_results.append(np.mean(rf))\n",
    "    ensemble_results.append(np.mean(ensemble))\n",
    "\n",
    "\n",
    "print(\"XGBoost: \", np.mean(xgboost_results), np.std(xgboost_results))\n",
    "print(\"SVM: \", np.mean(svm_results), np.std(svm_results))\n",
    "print(\"RF: \", np.mean(rf_results), np.std(rf_results))\n",
    "print(\"Ensemble: \", np.mean(ensemble_results), np.std(ensemble_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb score on validation set: 6500.0\n",
      "svm score on validation set: 6650.0\n",
      "rf score on validation set: 6400.0\n",
      "Ensemble score on validation set: 6450.0\n"
     ]
    }
   ],
   "source": [
    "model1 = xgb.XGBClassifier(**xgboost_params)\n",
    "model1.fit(train_set_X_with_mds, train_set_y)\n",
    "\n",
    "model2 = SVC(probability=True, **svm_params)\n",
    "model2.fit(train_set_X_with_mds, train_set_y)\n",
    "\n",
    "model3 = RandomForestClassifier(**rf_params)\n",
    "model3.fit(train_set_X_with_mds, train_set_y)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions1 = model1.predict_proba(val_set_X_with_mds)[:, 1]\n",
    "predictions2 = model2.predict_proba(val_set_X_with_mds)[:, 1]\n",
    "predictions3 = model3.predict_proba(val_set_X_with_mds)[:, 1]\n",
    "\n",
    "# Combine the predictions\n",
    "final_predictions_proba = (predictions1 + predictions2 + predictions3) / 3\n",
    "final_score = default_competition_metric(\n",
    "    y_val, y_pred_proba=final_predictions_proba, k=number_of_features\n",
    ")\n",
    "print(\n",
    "    f\"xgb score on validation set: {default_competition_metric(y_val, y_pred_proba=predictions1, k=number_of_features)}\"\n",
    ")\n",
    "print(\n",
    "    f\"svm score on validation set: {default_competition_metric(y_val, y_pred_proba=predictions2, k=number_of_features)}\"\n",
    ")\n",
    "print(\n",
    "    f\"rf score on validation set: {default_competition_metric(y_val, y_pred_proba=predictions3, k=number_of_features)}\"\n",
    ")\n",
    "print(f\"Ensemble score on validation set: {final_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual models\n",
    "### testing on features [100, 102, 105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"../data/x_train.npy\")\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "X_val = np.load(\"../data/x_val.npy\")\n",
    "y_val = np.load(\"../data/y_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    \"n_estimators\": 1600,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 4,\n",
    "    \"max_features\": \"log2\",\n",
    "    \"max_depth\": 10,\n",
    "    \"bootstrap\": False,\n",
    "}\n",
    "svm_params = {\n",
    "    'kernel': 'rbf',\n",
    "    'gamma': 0.01,\n",
    "    'C': 1,\n",
    "}\n",
    "xgboost_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.01,\n",
    "    'colsample_bytree': 0.75,\n",
    "}\n",
    "features = [100, 102, 105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:, features]\n",
    "X_val = X_val[:, features]\n",
    "number_of_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_X = X_train\n",
    "train_set_y = y_train\n",
    "val_set_X = X_val\n",
    "val_set_y = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(\n",
    "    n_components=1,\n",
    "    random_state=21,\n",
    "    n_init=6,\n",
    "    normalized_stress=False,\n",
    "    eps=0.0001,\n",
    "    max_iter=600,\n",
    ")\n",
    "# this step should be calculated in every cross-validation fold so 5 * 10 = 50 times\n",
    "# but it takes a lot of time so we will do it only once and our results will be biased\n",
    "# but in validation it is done by the book\n",
    "train_mds = mds.fit_transform(train_set_X)\n",
    "val_mds = mds.fit_transform(val_set_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_X_with_mds = np.concatenate([train_set_X, train_mds], axis=1)\n",
    "val_set_X_with_mds = np.concatenate([val_set_X, val_mds], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing on cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m model2\u001b[38;5;241m.\u001b[39mfit(X_train_cv_with_mds, y_train_cv)\n\u001b[0;32m     32\u001b[0m model3 \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrf_params)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mmodel3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_cv_with_mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m predictions1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mpredict_proba(X_val_cv_with_mds)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     36\u001b[0m predictions2 \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mpredict_proba(X_val_cv_with_mds)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tymot\\Documents\\studia\\aml\\pro\\adv_ml_project_2\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tymot\\Documents\\studia\\aml\\pro\\adv_ml_project_2\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\tymot\\Documents\\studia\\aml\\pro\\adv_ml_project_2\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tymot\\Documents\\studia\\aml\\pro\\adv_ml_project_2\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\tymot\\Documents\\studia\\aml\\pro\\adv_ml_project_2\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\tymot\\Documents\\studia\\aml\\pro\\adv_ml_project_2\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tymot\\Documents\\studia\\aml\\pro\\adv_ml_project_2\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:200\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    192\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    193\u001b[0m         X,\n\u001b[0;32m    194\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32mc:\\Users\\tymot\\Documents\\studia\\aml\\pro\\adv_ml_project_2\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seeds = [21, 22, 23, 24, 25, 5111, 23525, 34934, 343243]\n",
    "rf_results = []\n",
    "svm_results = []\n",
    "xgboost_results = []\n",
    "ensemble_results = []\n",
    "\n",
    "for num, seed in enumerate(seeds):\n",
    "    cv_folds = 5\n",
    "    np.random.seed(seed)\n",
    "    idx = np.array(range(X_train.shape[0]))\n",
    "    np.random.shuffle(idx)\n",
    "    rf = []\n",
    "    svm = []\n",
    "    xgboost = []\n",
    "    ensemble = []\n",
    "\n",
    "    idx = idx % cv_folds\n",
    "    for j in range(cv_folds):\n",
    "        X_train_cv = train_set_X[idx != j]\n",
    "        X_val_cv = train_set_X[idx == j]\n",
    "        X_train_cv_with_mds = train_set_X_with_mds[idx != j]\n",
    "        X_val_cv_with_mds = train_set_X_with_mds[idx == j]\n",
    "        y_train_cv = y_train[idx != j]\n",
    "        y_val_cv = y_train[idx == j]\n",
    "\n",
    "        model1 = xgb.XGBClassifier(**xgboost_params)\n",
    "        model1.fit(X_train_cv_with_mds, y_train_cv)\n",
    "\n",
    "        model2 = SVC(probability=True, **svm_params)\n",
    "        model2.fit(X_train_cv_with_mds, y_train_cv)\n",
    "\n",
    "        model3 = RandomForestClassifier(**rf_params)\n",
    "        model3.fit(X_train_cv_with_mds, y_train_cv)\n",
    "\n",
    "        predictions1 = model1.predict_proba(X_val_cv_with_mds)[:, 1]\n",
    "        predictions2 = model2.predict_proba(X_val_cv_with_mds)[:, 1]\n",
    "        predictions3 = model3.predict_proba(X_val_cv_with_mds)[:, 1]\n",
    "        final_predictions_proba = (predictions1 + predictions2 + predictions3) / 3\n",
    "\n",
    "        xgboost.append(\n",
    "            default_competition_metric(\n",
    "                y_val_cv, y_pred_proba=predictions1, k=number_of_features\n",
    "            )\n",
    "        )\n",
    "        svm.append(\n",
    "            default_competition_metric(\n",
    "                y_val_cv, y_pred_proba=predictions2, k=number_of_features\n",
    "            )\n",
    "        )\n",
    "        rf.append(\n",
    "            default_competition_metric(\n",
    "                y_val_cv, y_pred_proba=predictions3, k=number_of_features\n",
    "            )\n",
    "        )\n",
    "        ensemble.append(\n",
    "            default_competition_metric(\n",
    "                y_val_cv, y_pred_proba=final_predictions_proba, k=number_of_features\n",
    "            )\n",
    "        )\n",
    "\n",
    "    xgboost_results.append(np.mean(xgboost))\n",
    "    svm_results.append(np.mean(svm))\n",
    "    rf_results.append(np.mean(rf))\n",
    "    ensemble_results.append(np.mean(ensemble))\n",
    "\n",
    "\n",
    "print(\"XGBoost: \", np.mean(xgboost_results), np.std(xgboost_results))\n",
    "print(\"SVM: \", np.mean(svm_results), np.std(svm_results))\n",
    "print(\"RF: \", np.mean(rf_results), np.std(rf_results))\n",
    "print(\"Ensemble: \", np.mean(ensemble_results), np.std(ensemble_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb score on validation set: 6850.0\n",
      "svm score on validation set: 7000.0\n",
      "rf score on validation set: 6750.0\n",
      "Ensemble score on validation set: 6850.0\n"
     ]
    }
   ],
   "source": [
    "model1 = xgb.XGBClassifier(**xgboost_params)\n",
    "model1.fit(train_set_X, train_set_y)\n",
    "\n",
    "model2 = SVC(probability=True, **svm_params)\n",
    "model2.fit(train_set_X, train_set_y)\n",
    "\n",
    "model3 = RandomForestClassifier(**rf_params)\n",
    "model3.fit(train_set_X_with_mds, train_set_y)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions1 = model1.predict_proba(val_set_X)[:, 1]\n",
    "predictions2 = model2.predict_proba(val_set_X)[:, 1]\n",
    "predictions3 = model3.predict_proba(val_set_X_with_mds)[:, 1]\n",
    "\n",
    "# Combine the predictions\n",
    "final_predictions_proba = (predictions1 + predictions2 + predictions3) / 3\n",
    "final_score = default_competition_metric(\n",
    "    y_val, y_pred_proba=final_predictions_proba, k=number_of_features\n",
    ")\n",
    "print(\n",
    "    f\"xgb score on validation set: {default_competition_metric(y_val, y_pred_proba=predictions1, k=number_of_features)}\"\n",
    ")\n",
    "print(\n",
    "    f\"svm score on validation set: {default_competition_metric(y_val, y_pred_proba=predictions2, k=number_of_features)}\"\n",
    ")\n",
    "print(\n",
    "    f\"rf score on validation set: {default_competition_metric(y_val, y_pred_proba=predictions3, k=number_of_features)}\"\n",
    ")\n",
    "print(f\"Ensemble score on validation set: {final_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set_X_with_mds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_original = np.loadtxt(\"../data/x_train.txt\")\n",
    "y_train_original = np.loadtxt(\"../data/y_train.txt\")\n",
    "x_test_original = np.loadtxt(\"../data/x_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_original = x_train_original[:, features]\n",
    "x_test_original = x_test_original[:, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(\n",
    "    n_components=1,\n",
    "    random_state=21,\n",
    "    n_init=6,\n",
    "    normalized_stress=False,\n",
    "    eps=0.0001,\n",
    "    max_iter=600,\n",
    ")\n",
    "train_original_mds = mds.fit_transform(x_train_original)\n",
    "test_original_mds = mds.fit_transform(x_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_original_with_mds = np.concatenate(\n",
    "    [x_train_original, train_original_mds], axis=1\n",
    ")\n",
    "x_test_original_with_mds = np.concatenate([x_test_original, test_original_mds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = xgb.XGBClassifier(**xgboost_params)\n",
    "model1.fit(x_train_original, y_train_original)\n",
    "\n",
    "model2 = SVC(probability=True, **svm_params)\n",
    "model2.fit(x_train_original, y_train_original)\n",
    "\n",
    "model3 = RandomForestClassifier(**rf_params)\n",
    "model3.fit(x_train_original_with_mds, y_train_original)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions1 = model1.predict_proba(x_test_original)[:, 1]\n",
    "predictions2 = model2.predict_proba(x_test_original)[:, 1]\n",
    "predictions3 = model3.predict_proba(x_test_original_with_mds)[:, 1]\n",
    "\n",
    "# Combine the predictions\n",
    "final_predictions_proba = (predictions1 + predictions2 + predictions3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(final_predictions_proba)\n",
    "top_02 = np.argsort(final_predictions_proba)[::-1][: n // 5]\n",
    "y_pred = np.zeros(n)\n",
    "y_pred[top_02] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should equal to 1000\n",
    "sum(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(final_predictions, columns_used):\n",
    "    \"\"\"\n",
    "    final_predictions: binary predictions for the test set\n",
    "    columns_used: list of column indexes used in the models\n",
    "\n",
    "    This function should create a submission file for the test set\n",
    "    \"\"\"\n",
    "    final_predictions = np.where(final_predictions == 1)[0]\n",
    "    final_predictions = final_predictions + 1\n",
    "    columns_used = [val + 1 for val in columns_used]\n",
    "    print(f\"You predicted as positive {len(final_predictions)} samples\")\n",
    "    print(f\"You used: {len(columns_used)} columns\")\n",
    "    pd.DataFrame(final_predictions).to_csv(\"320637_obs.txt\", header=False, index=False)\n",
    "    pd.DataFrame(columns_used).to_csv(\"320637_vars.txt\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission_file(y_pred, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asseco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
